---
title: "Modeling Gender Interests"
output: html_notebook
---

# Introduction 
We chose to investigate a “Young People Survey” data set. This data set has 150 variables representing answers to survey questions that were answered in both paper and electronic form. This survey was given to young Slovakian people between the ages of 15-30. The Survey asked them to give ratings to questions about different subjects pertinent to young people. The variables can be split into some distinct subsets as defined on Kaggle:

* Music preferences (19 items)
* Movie preferences (12 items)
* Hobbies & interests (32 items)
* Phobias (10 items)
* Health habits (3 items)
* Personality traits, views on life, & opinions (57 items)
* Spending habits (7 items)
* Demographics (10 items)

Answers to questions, for the most part, were recorded on a  5 point likert scale.  In this analysis we aimed to pinpoint a few variables that contribute most, to predicting the gender variable, using the Hobbies & interests subset. In all models presented in this paper, we predict the female gender in the dataset. Models were fitted and evaluated with the caret package.  Throughout our modeling process we estimated test error with 10 fold cross validation and then evaluated test error on a held out test set containing 20% of the data. We predefined fold indexes to avoid leakage, this was the main reason we used caret. 


```{r, echo=FALSE}
library(car)
library(GGally)
library(lime)
library(caret)
library(readr)
library(rpart)
library(d3heatmap)
library(RColorBrewer)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(randomForest)
library(data.table)
library(highcharter)
library(tidyr)
library(factoextra)
setwd("~/Documents/Rscripts /Stat 295/Final")
youngPeople=read.csv("responses.csv", stringsAsFactors = F)
variables = names(youngPeople)
names(youngPeople)[2] = "song speed"
names(youngPeople)[10] = "Metal/HardRock"
#youngPeople$Gender = as.factor(youngPeople$Gender)
youngPeople$Alcohol[youngPeople$Alcohol == "never"] = 1
youngPeople$Alcohol[youngPeople$Alcohol == "social drinker"] = 2
youngPeople$Alcohol[youngPeople$Alcohol == "drink a lot"] = 3
youngPeople$Alcohol = as.numeric(youngPeople$Alcohol)
youngPeople$Smoking[youngPeople$Smoking == "never smoked"] = 1
youngPeople$Smoking[youngPeople$Smoking == "tried smoking"] = 2
youngPeople$Smoking[youngPeople$Smoking == "former smoker"] = 3
youngPeople$Smoking[youngPeople$Smoking == "current smoker"] = 4
youngPeople$Smoking = as.numeric(youngPeople$Smoking)
youngPeople$Internet.usage[youngPeople$Internet.usage == "no time at all"] = 1
youngPeople$Internet.usage[youngPeople$Internet.usage == "less than an hour a day"] = 2
youngPeople$Internet.usage[youngPeople$Internet.usage == "few hours a day"] = 3
youngPeople$Internet.usage[youngPeople$Internet.usage == "most of the day"] = 4
youngPeople$Internet.usage = as.integer(youngPeople$Internet.usage)
youngPeople$Punctuality[youngPeople$Punctuality == "i am often early"] = 3
youngPeople$Punctuality[youngPeople$Punctuality == "i am always on time"] = 2
youngPeople$Punctuality[youngPeople$Punctuality == "i am often running late"] = 1
youngPeople$Punctuality[youngPeople$Punctuality == ""] = NA
youngPeople$Punctuality = as.numeric(youngPeople$Punctuality)
youngPeople$Lying[youngPeople$Lying == "never"] = 1
youngPeople$Lying[youngPeople$Lying == "only to avoid hurting someone"] = 2
youngPeople$Lying[youngPeople$Lying == "sometimes"] = 3
youngPeople$Lying[youngPeople$Lying == "everytime it suits me"] = 4
youngPeople$Lying[youngPeople$Lying == ""] = NA
youngPeople$Lying = as.numeric(youngPeople$Lying)
library("FactoMineR")
```

# Modeling set up and EDA 

This piece here is very important.  
* I first divide my data into a training and test set.  We want to be able to evalutate the true  test error rate of the model.  We are not able to do this if we evaluate the model performance on data that was used to train it.
* The data will be split into a 80% training set and a 20% testing set.
* In order for our cross-valadation results to be valid, we need to make sure that we everalutate models on the same folds.
```{r}
split = round(nrow(youngPeople) * .80)
train = youngPeople[1:split, ]
test = youngPeople[(split + 1):nrow(youngPeople), ]
interests = train %>% select(Gender,History:Pets) %>% filter(Gender != "") %>% drop_na() %>% mutate(Gender = as.factor(Gender))
interestsTest = test %>% select(Gender,History:Pets) %>% filter(Gender != "") %>% drop_na() %>% mutate(Gender = as.factor(Gender))
```
## Variable selection:

In order to select variables for our models, we used two different methods, best subset regression and principal component analysis.

#### PCA:

Principal component analysis is a technique used for dimensionality redecution. I really like this definition from Abbas Keshvani: https://coolstatsblog.com/author/mabbask/

* "Principal Component Analysis, or PCA, is a statistical method used to reduce the number of variables in a dataset. It does so by lumping highly correlated variables together. Naturally, this comes at the expense of accuracy. However, if you have 50 variables and realize that 40 of them are highly correlated, you will gladly trade a little accuracy for simplicity."

Principal component analysis was how we eventaully decided on the interestes subset of this dataset.  We modeled the principal component analysis in many different subsets and found this one most interesting.  

In the following figure, factor loadings are colored by the Gender of the observation.  Principal component analysis is an unsupervised learning technqiue, which means that it does NOT use these Gender labels when reducing dimensions. 

```{r, fig.height=7, fig.width=7, echo=FALSE}
h=PCA(interests[,2:ncol(interests)], scale = T, graph = F) 
fviz_pca_biplot(h,geom = "point",
                repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                habillage = interests$Gender,
                col.ind = "#696567" # Individuals color
                )
  
```
#### Best Subset Regression 
In this course we studied the great book *Introduction to Statistical Learning with Applications in R* by Gareth et al.  The book spends a good chunk on this method of variable selection.  Personally, it is not my favorite method.  We have come up with cross valaidation, regularization and many other ways to better handle this problem. We will use this in exploratory models to develop an inital hypothesis. While exploring we tried both forward and backward selection. Results were similar, so we decided to simply use forward selection.

This first figure will not include the nvmax argument to regsubsets(). This argument selects the maximum subsets to examine.  The deault for this argument 8.
```{r}
library(leaps)
regfit <- regsubsets(Gender ~ ., interests, method = "forward")
```
```{r, fig.height=10, fig.width= 10, echo=FALSE}
par(mfrow=c(2,2))
plot(regfit, scale = 'r2')
plot(regfit, scale = 'adjr2')
plot(regfit, scale = 'Cp')
plot(regfit, scale = 'bic')
```
I next use the settings reccomended by ISLR, setting nvmax equal to the number of variables.
```{r}
library(leaps)
regfit2 <- regsubsets(Gender ~ ., interests, method = "forward", nvmax = 33)
```
```{r, fig.height=11, fig.width=10, echo=FALSE}
par(mfrow=c(2,2))
plot(regfit2, scale = 'r2')
plot(regfit2, scale = 'adjr2')
plot(regfit2, scale = 'Cp')
plot(regfit2, scale = 'bic')
```


## Modeling
for creating statistical models we decided to use the caret packaage.  It provides a variaty of tools to help us access both crossvalidation and test error.

I next createa a train control for all of the models.  This allows us to use the same folds and crossvalidation methods across models.
```{r}
myFolds <- createFolds(interests$Gender, k = 10)
set.seed(2017)

myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!
  index = myFolds
  
)
pred = function(model){
preds = predict(model, interestsTest)
confusionMatrix(interestsTest$Gender, preds)}

```
#### Initial model: Logistic Regression
We fit an initial logistic regression with variables selected as important by both our stepwise selection and PCA.  This model performs alright, but their is a big issue with this model.

```{r}
logreg<- train(Gender ~ PC + Reading + Cars + Shopping, interests, method = 'glm',
              trControl = myControl)
pred(logreg)
```


```{r, warning=FALSE, echo=FALSE, fig.align='center'}
logplot = ggcoef(logreg$finalModel,  errorbar_color = "blue",
  errorbar_height = .25, exclude_intercept = T) + ggtitle("Initial Logistic Regression Model")
```
#### It looks like there is some issues with multicolinearity.  Remember we are predicting the female gender.

```{r, warning=FALSE, echo=FALSE, fig.align='center'}
library(gridExtra)
a=youngPeople %>% filter(Gender != "") %>% ggplot(aes(Shopping, fill = Gender))+
  geom_bar()
b=youngPeople %>% filter(Gender != "") %>% ggplot(aes(Cars, fill = Gender))+
  geom_bar()
c=youngPeople %>% filter(Gender != "") %>% ggplot(aes(PC, fill = Gender))+
  geom_bar()
d=youngPeople %>% filter(Gender != "") %>% ggplot(aes(Reading, fill = Gender))+
  geom_bar()
grid.arrange(a,d,c,b)
```


```{r, echo=FALSE, fig.height=3, fig.width=6, fig.align="center"}
logplot
```
some notes about some of the caret measures:
https://stats.stackexchange.com/questions/124001/what-is-the-intuition-behind-the-kappa-statistical-value-in-classification
https://stats.stackexchange.com/questions/316641/what-is-the-usefulness-of-detection-rate-in-a-confusion-matrix

#### follow up models:

#### Random Forest
```{r}
Tree <- train(
  Gender ~ .,
  tuneLength = 10,
  importance = "impurity",
  data = interests, method = "ranger",
  trControl = myControl)
pred(Tree)
Tree
```
# as far as tree based methods go it seems that extra trees is doing really well. 
https://www.quora.com/What-is-the-extra-trees-algorithm-in-machine-learning
```{r}
treeinfo = as.data.frame(Tree$results)
ggplot(treeinfo, aes(mtry, ROC, color = splitrule))+
         geom_point()+
        ggtitle("Tree Rules")
```
```{r, warning = FALSE}
IMP= function(model){
importance = as.data.frame(varImp(model)$importance) %>% add_rownames("Question") 
#%>% mutate(Question = as.factor(Question))
importance = arrange(importance, desc(Overall)) %>% mutate(Question = as.factor(Question))
order = importance$Question
importance$Question = factor(importance$Question, levels = order)
return(importance)
}

```
```{r}
TreeImportant= IMP(Tree)
```

```{r}
implot= function(importance){
ggplot(importance, aes(Question, Overall))+
  geom_bar(stat = "identity")+
  coord_flip()}
a=TreeImportant %>% implot + ggtitle("Random Forest")
a
```
```{r}
logreg2 = train(Gender ~ Cars + PC + Shopping + Reading + Dancing, interests, method = 'glm',
              trControl = myControl)
pred(logreg2)
```
Notes of Caret Variable Importance: http://ftp.uni-bayreuth.de/math/statlib/R/CRAN/doc/vignettes/caret/caretVarImp.pdf

Here’s more info on the extra trees algorithm https://www.quora.com/What-is-the-extra-trees-algorithm-in-machine-learning
## What This is doing is crazy, this is really one of the best estimate of out of sample error possible with the data.
```{r}
library(lars)
ridge <- train(Gender ~., data = interests,
               method='glmnet', 
               preProcess=c('scale', 'center'),
               trControl = myControl,
               tuneGrid = expand.grid(alpha = seq(.05, 1, length = 15),
                                               lambda = c((1:5)/10)))

ridgeImportance = IMP(ridge)
b=ridgeImportance %>% implot()+ ggtitle("Lasso Regression")
pred(ridge)
varImp(ridge, scale = F)
```

```{r}
pred(ridge)
```
## This is a really cool idea, these are scaled absolute value t-statistics. Were able to get some sort of way to compare what the two models are saying.
```{r, warning= FALSE}
fullmodel = train(Gender ~., data= interests, method = 'glm',
              trControl = myControl)
c=IMP(fullmodel) %>% implot() + ggtitle("Logistic Regression")
pred(fullmodel)
varImp(logreg, scale =FALSE )
```
```{r, fig.height= 12, fig.width=14}
library(gridExtra)
grid.arrange(a,b,c) 
```

